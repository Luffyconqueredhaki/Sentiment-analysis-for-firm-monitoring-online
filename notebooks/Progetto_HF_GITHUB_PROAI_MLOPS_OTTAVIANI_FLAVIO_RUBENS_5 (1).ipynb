{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6wWUlWpV7FU"
      },
      "outputs": [],
      "source": [
        "# CELLA 1: Setup e Import\n",
        "\n",
        "!pip install -q datasets emoji\n",
        "\n",
        "import os, zipfile, requests, io, re, random\n",
        "import pandas as pd, numpy as np\n",
        "import emoji, html\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.special import softmax\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU disponibile\")\n",
        "\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "SAMPLE_SIZE = 10000\n",
        "MAX_LEN = 128\n",
        "BATCH = 16\n",
        "LOW_CONF_THRESHOLD = 0.60\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download e Preprocessing Dataset\n",
        "\n",
        "zip_url = \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
        "data_dir = \"./sentiment140_data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "csv_path = os.path.join(data_dir, \"training.1600000.processed.noemoticon.csv\")\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    r = requests.get(zip_url, timeout=120)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall(data_dir)\n",
        "\n",
        "cols = [\"target\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
        "df = pd.read_csv(csv_path, encoding=\"latin-1\", names=cols)\n",
        "df = df[[\"target\",\"text\"]]\n",
        "# Mappa 0->0 (Negativo), 4->1 (Positivo)\n",
        "df[\"target\"] = df[\"target\"].map({0:0, 4:1})\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "def preprocess_social(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = html.unescape(text)\n",
        "    t = re.sub(r\"@\\w+\", \"@user\", t)\n",
        "    t = re.sub(r\"http\\S+|www\\.\\S+\", \"http\", t)\n",
        "    t = re.sub(r\"#(\\w+)\", r\"\\1\", t)\n",
        "    t = emoji.demojize(t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "df[\"text_clean\"] = df[\"text\"].astype(str).apply(preprocess_social)\n",
        "df = df.groupby(\"target\").apply(lambda x: x.sample(min(len(x), SAMPLE_SIZE//2), random_state=42)).reset_index(drop=True)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df[\"target\"])\n",
        "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
        "print(f\"Distribuzione: {df['target'].value_counts().sort_index().to_dict()}\")"
      ],
      "metadata": {
        "id": "Z9tKJIM2V_YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizzazione\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text_clean\",\"target\"]].rename(columns={\"text_clean\":\"text\", \"target\":\"labels\"}))\n",
        "test_ds = Dataset.from_pandas(test_df[[\"text_clean\",\"target\"]].rename(columns={\"text_clean\":\"text\", \"target\":\"labels\"}))\n",
        "\n",
        "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
        "test_ds = test_ds.map(tokenize_batch, batched=True)\n",
        "\n",
        "train_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
        "test_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])"
      ],
      "metadata": {
        "id": "H8E1HB8WV_bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "# Questo dice di scartare la vecchia testa di classificazione (3 etichette)\n",
        "# e inizializzarne una nuova (2 etichette) per il task.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True #IGNORA I NEUTRALI\n",
        ")\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./hf_retrained_model\",\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    per_device_eval_batch_size=BATCH,\n",
        "    num_train_epochs=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=50,\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=test_ds, compute_metrics=compute_metrics)\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./hf_retrained_model\")\n",
        "\n",
        "# Plot Training Loss\n",
        "train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
        "steps_train = [log['step'] for log in trainer.state.log_history if 'loss' in log]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps_train, train_losses, 'b-', linewidth=2, marker='o', markersize=4)\n",
        "plt.xlabel('Step', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== TRAINING SUMMARY ===\")\n",
        "print(f\"Initial Loss: {train_losses[0]:.4f}\")\n",
        "print(f\"Final Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Loss Reduction: {100*(train_losses[0]-train_losses[-1])/train_losses[0]:.1f}%\")"
      ],
      "metadata": {
        "id": "atn2VusnkfBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Base\n",
        "\n",
        "pred_output = trainer.predict(test_ds)\n",
        "logits = pred_output.predictions\n",
        "probs = softmax(logits, axis=1)\n",
        "preds = np.argmax(probs, axis=1)\n",
        "labels = pred_output.label_ids\n",
        "\n",
        "acc = accuracy_score(labels, preds)\n",
        "f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")\n",
        "print(\"\\n\", classification_report(labels, preds, target_names=['Negativo', 'Positivo'], zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negativo', 'Positivo'], yticklabels=['Negativo', 'Positivo'])\n",
        "plt.title(\"Confusion Matrix\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Predetto\", fontsize=12)\n",
        "plt.ylabel(\"Vero\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Low confidence\n",
        "low_conf = []\n",
        "for i in range(min(20, len(probs))):\n",
        "    top_score = float(np.max(probs[i]))\n",
        "    if top_score < LOW_CONF_THRESHOLD:\n",
        "        low_conf.append((i, int(labels[i]), int(np.argmax(probs[i])), top_score))\n",
        "\n",
        "if low_conf:\n",
        "    print(f\"\\nLow confidence (<{LOW_CONF_THRESHOLD}): {len(low_conf)}\")"
      ],
      "metadata": {
        "id": "LawT_0F3fE1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il modello ha sbagliato quasi allo stesso modo per entrambe: ha confuso 123 Negativi per Positivi e 117 Positivi per Negativi.\n",
        "\n",
        "Questo bilanciamento è OTTIMO Significa che il modello non è sbilanciato: non ha una preferenza nel classificare tutto come \"Positivo\" o \"Negativo\""
      ],
      "metadata": {
        "id": "xuB1Uuf_l-Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline Inferenza\n",
        "\n",
        "# Diciamo al modello che l'etichetta 0 è \"negative\" e l'etichetta 1 è \"positive\"\n",
        "#sovrascrive la vecchia configurazione {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "print(\"Aggiornamento configurazione modello per 2 etichette...\")\n",
        "model.config.id2label = { 0: 'negative', 1: 'positive' }\n",
        "model.config.label2id = { 'negative': 0, 'positive': 1 }\n",
        "print(\"Configurazione aggiornata.\")\n",
        "\n",
        "#Crea la pipeline\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Aggiorna la label_map\n",
        "# Ora la pipeline restituirà solo \"negative\" o \"positive\".\n",
        "# La classe \"neutral\" non esiste più per questo modello.\n",
        "label_map = {\n",
        "    \"negative\": \"Negativo\",\n",
        "    \"positive\": \"Positivo\"\n",
        "    # \"neutral\" non è più necessario\n",
        "}\n",
        "\n",
        "print(\"Pipeline di inferenza pronta e configurata correttamente.\")"
      ],
      "metadata": {
        "id": "0gIMJOIWcfSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test su 4 Tweet Random\n",
        "predictions_list = []\n",
        "random_samples = test_df.sample(4)\n",
        "\n",
        "for index, row in random_samples.iterrows():\n",
        "    text_to_analyze = row['text_clean']\n",
        "    result = sentiment_pipeline(text_to_analyze)[0]\n",
        "    predicted_label = label_map.get(result['label'], 'Sconosciuto')\n",
        "    predicted_score = result['score']\n",
        "    print(f\"\\nTweet {index}:\")\n",
        "    print(f\"Testo: {text_to_analyze}\")\n",
        "    print(f\"Sentiment: {predicted_label} (Confidenza: {predicted_score:.3f})\")\n",
        "    predictions_list.append(predicted_label)"
      ],
      "metadata": {
        "id": "eLxK8ywQV_lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grafico 4 Tweet\n",
        "\n",
        "results_series = pd.Series(predictions_list)\n",
        "sentiment_counts = results_series.value_counts()\n",
        "color_map = {'Negativo': '#FF6347', 'Neutrale': '#B0C4DE', 'Positivo': '#90EE90'}\n",
        "ordered_counts = sentiment_counts.reindex(['Negativo', 'Neutrale', 'Positivo'], fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "ordered_counts.plot(kind='bar', color=[color_map.get(label, 'blue') for label in ordered_counts.index])\n",
        "plt.title('Sentiment 4 Tweet Casuali')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.yticks(range(int(ordered_counts.max()) + 2))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJedk0UKV_nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grafico su TUTTO il Test Set\n",
        "\n",
        "print(f\"\\n=== GRAFICO RIASSUNTIVO SU TUTTO IL TEST SET ({len(preds)} TWEET) ===\")\n",
        "\n",
        "# --- ECCO LA CORREZIONE ---\n",
        "# La mappa deve riflettere il NOSTRO modello (0=Neg, 1=Pos)\n",
        "numeric_label_map = {\n",
        "    0: \"Negativo\",\n",
        "    1: \"Positivo\"\n",
        "    # 1 NON è Neutrale\n",
        "}\n",
        "\n",
        "# Converti l'array numpy 'preds' in una Serie di pandas\n",
        "preds_series = pd.Series(preds)\n",
        "mapped_preds = preds_series.map(numeric_label_map)\n",
        "sentiment_counts = mapped_preds.value_counts()\n",
        "\n",
        "# Prepara per il plot\n",
        "color_map = {'Negativo': '#FF6347', 'Neutrale': '#B0C4DE', 'Positivo': '#90EE90'}\n",
        "# Assicurati che l'ordine sia corretto per le nostre etichette\n",
        "ordered_counts = sentiment_counts.reindex(['Negativo', 'Neutrale', 'Positivo'], fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "bars = ordered_counts.plot(\n",
        "    kind='bar',\n",
        "    color=[color_map.get(label, 'blue') for label in ordered_counts.index]\n",
        ")\n",
        "\n",
        "plt.title(f'Distribuzione Sentiment sull\\'Intero Test Set ({len(preds)} Tweet)')\n",
        "plt.xlabel('Sentiment Predetto')\n",
        "plt.ylabel('Numero di Tweet')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for bar in bars.patches:\n",
        "    plt.annotate(format(bar.get_height(), '.0f'),\n",
        "                 (bar.get_x() + bar.get_width() / 2,\n",
        "                  bar.get_height()), ha='center', va='center',\n",
        "                 size=10, xytext=(0, 8),\n",
        "                 textcoords='offset points')\n",
        "\n",
        "plt.ylim(top=ordered_counts.max() * 1.15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-zUhpgd3WOVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grafici delle Curve di Validazione\n",
        "\n",
        "print(\"\\n=== GRAFICI ANDAMENTO VALIDAZIONE ===\")\n",
        "\n",
        "#Estrai tutti i log dalla cronologia del trainer\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "#Separa i log di training da quelli di valutazione (eval)\n",
        "# Cerca solo i dizionari che contengono 'eval_loss'\n",
        "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
        "\n",
        "if not eval_logs:\n",
        "    print(\"Nessun log di valutazione trovato.\")\n",
        "    print(\"Assicurati che 'evaluation_strategy' e 'eval_steps' siano impostati in TrainingArguments.\")\n",
        "else:\n",
        "    #Estrae i valori per gli assi X e Y\n",
        "    steps = [log['step'] for log in eval_logs]\n",
        "    eval_losses = [log['eval_loss'] for log in eval_logs]\n",
        "    eval_accuracies = [log['eval_accuracy'] for log in eval_logs]\n",
        "\n",
        "    #Crea i grafici (due grafici in una sola figura)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Grafico 1: Validation Loss\n",
        "    ax1.plot(steps, eval_losses, marker='o', linestyle='--', color='red')\n",
        "    ax1.set_title('Validation Loss vs. Steps', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Steps')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Grafico 2: Validation Accuracy\n",
        "    ax2.plot(steps, eval_accuracies, marker='o', linestyle='--', color='green')\n",
        "    ax2.set_title('Validation Accuracy vs. Steps', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Steps')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yR0E6rQIWP4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "71dfDWqnojr1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}